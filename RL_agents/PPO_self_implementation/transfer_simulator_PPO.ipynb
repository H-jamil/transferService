{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d346df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback,CheckpointCallback,CallbackList\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f56c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes.pickle', 'rb') as handle:\n",
    "    loaded_dfs = pickle.load(handle)\n",
    "with open('initial_dataframes.pickle', 'rb') as handle:\n",
    "    loaded_initial_dfs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_dfs is the the dfs that has (previous_action,action) single action based logs \n",
    "print_empty_dataframes(loaded_dfs) \n",
    "\n",
    "print('\\n\\n\\n')\n",
    "#loaded_dfs is the the dfs that has only action one single action based logs \n",
    "print_empty_dataframes(loaded_initial_dfs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=transferClass(loaded_dfs,loaded_initial_dfs,'random')\n",
    "evaluation_env=transferClass(loaded_dfs,loaded_initial_dfs,'random')\n",
    "total_scores=0\n",
    "total_scores_round=0\n",
    "s = env.reset()\n",
    "action_list=[]\n",
    "reward_list=[]\n",
    "reward_list_round=[]\n",
    "done = False\n",
    "while not done:\n",
    "    a=env.action_space.sample()\n",
    "    if a==0:\n",
    "        a=1\n",
    "    s_next, r, done, info = env.step(a)\n",
    "    action_list.append(a)\n",
    "    reward_list.append(r)\n",
    "    total_scores += r\n",
    "    total_scores_round += round(r,3)\n",
    "    reward_list_round.append(round(r,3))\n",
    "    s = s_next    \n",
    "env.close()\n",
    "print(f\"Total Reward: {total_scores}\")\n",
    "print(f\"Total Reward round: {total_scores_round}\")\n",
    "print(f\"actions {action_list},   {len(action_list)}\")\n",
    "print(f\"rewards {reward_list},  {len(reward_list)}\")\n",
    "print(f\"rewards round {reward_list_round},  {len(reward_list_round)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=transferClass(loaded_dfs,loaded_initial_dfs,'random')\n",
    "evaluation_env=transferClass(loaded_dfs,loaded_initial_dfs,'random')\n",
    "\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,net_arch=[{'pi': [128, 128], 'vf': [128, 128]}])\n",
    "model = PPO(\"MlpPolicy\", env=env, policy_kwargs=policy_kwargs, verbose=1,tensorboard_log=\"./ppo_tensorboard/\",ent_coef=0.01)\n",
    "eval_callback = EvalCallback(evaluation_env, best_model_save_path='./ppo/ppo_best_model/',\n",
    "                               log_path='./ppo/ppo_logs/', eval_freq=1000,\n",
    "                               deterministic=True, render=False)\n",
    "# Callback for saving checkpoints every 1000 timesteps\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./ppo/ppo_checkpoints/',\n",
    "                                           name_prefix='ppo_model')\n",
    "\n",
    "# Combine both callbacks\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6757f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 1.4000000000000004\n",
      "actions [7, 5, 6, 7, 5, 6, 7, 5, 4, 6, 7, 5, 6, 7, 5, 6, 7, 7, 5, 6],   20\n",
      "rewards [1.0, 0.2, 4.2, -2.0, -3.4, 3.2, 1.2, -1.0, 1.4, -3.6, 1.0, -2.0, 2.0, 3.2, 0.2, -4.4, -0.2, 0.0, 5.0, -4.6],  20\n",
      "Average Throughput 3.1104000000000003\n",
      "Total Energy 6418.0\n",
      "Total packet loss 0.24979500000000002\n",
      "       Throughput  receiver_lr      Score         RTT     Energy   sender_lr\n",
      "count  100.000000        100.0  100.00000  100.000000  100.00000  100.000000\n",
      "mean     3.110400          0.0    2.71000   34.321000   64.18000    0.002498\n",
      "std      2.854597          0.0    2.51177    7.042028   31.17477    0.012905\n",
      "min      0.000000          0.0    0.00000    0.000000    0.00000    0.000000\n",
      "25%      0.640000          0.0    1.00000   32.700000   47.00000    0.000000\n",
      "50%      2.240000          0.0    2.00000   33.200000   73.00000    0.000000\n",
      "75%      5.120000          0.0    4.00000   37.375000   90.00000    0.000000\n",
      "max     11.200000          0.0   10.00000   48.500000  102.00000    0.117647\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"./ppo/ppo_best_model/best_model.zip\")\n",
    "done = False\n",
    "episode_reward = 0\n",
    "env=transferClass(loaded_dfs,loaded_initial_dfs,'random')\n",
    "action_list=[]\n",
    "reward_list=[]\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action_list.append(int(action))\n",
    "#     print(\"action: \",action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "#     obs = obs.astype(np.float32)\n",
    "#     print(\"obs: \", obs,\".... reward: \",reward)\n",
    "    reward_list.append(reward)\n",
    "    episode_reward += reward\n",
    "\n",
    "accumulator_df = pd.concat(env.obs_df)  # Add more DataFrames in the list if needed\n",
    "env.close()\n",
    "print(f\"Episode reward: {episode_reward}\")\n",
    "print(f\"actions {action_list},   {len(action_list)}\")\n",
    "print(f\"rewards {reward_list},  {len(reward_list)}\")\n",
    "print(f\"Average Throughput {accumulator_df['Throughput'].mean()}\")\n",
    "print(f\"Total Energy {accumulator_df['Energy'].sum()}\")\n",
    "print(f\"Total packet loss {accumulator_df['sender_lr'].sum()}\")\n",
    "print(accumulator_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transferService import transferService\n",
    "from optimizer_gd import *\n",
    "from transferClass import *\n",
    "import logging as log\n",
    "def get_env(env_string='transferService', optimizer='ppo_simulator_trained', REMOTE_IP = \"129.114.109.104\", REMOTE_PORT = \"80\", INTERVAL = 1,INTERFACE = \"eno1\",SERVER_IP = '127.0.0.1',SERVER_PORT = 8080):\n",
    "    for handler in log.root.handlers[:]:\n",
    "        log.root.removeHandler(handler)\n",
    "    log_FORMAT = '%(created)f -- %(levelname)s: %(message)s'\n",
    "    extraString=\"logFile\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = f\"./logFileDir/{optimizer}/\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    log_file = f\"logFileDir/{optimizer}/{optimizer}_{extraString}_{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "    log.basicConfig(\n",
    "        format=log_FORMAT,\n",
    "        datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "        level=log.INFO,\n",
    "        handlers=[\n",
    "            log.FileHandler(log_file),\n",
    "            # log.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    transfer_service = transferService(REMOTE_IP, REMOTE_PORT, INTERVAL, INTERFACE, SERVER_IP, SERVER_PORT, optimizer, log)\n",
    "    env = transferClass(transfer_service,optimizer)\n",
    "    return env\n",
    "\n",
    "class observationTranslator(gym.Wrapper):\n",
    "    def __init__(self, env, obs_min, obs_max, reward_scale=1.0):\n",
    "        super().__init__(env)\n",
    "        self.obs_min = obs_min\n",
    "        self.obs_max = obs_max\n",
    "        self.reward_scale = reward_scale\n",
    "        self.old_score = 0\n",
    "        self.score_difference_positive_threshold = 2\n",
    "        self.score_difference_negative_threshold = -2\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        return self.normalize_observation(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        normalized_obs = self.normalize_observation(observation)\n",
    "\n",
    "        if reward != 1000000:\n",
    "            score_difference = reward - self.old_score\n",
    "            self.old_score = reward\n",
    "            if score_difference > self.score_difference_positive_threshold:\n",
    "                # normalized_reward = 1\n",
    "                normalized_reward = max(-10, min(10, score_difference))\n",
    "            elif score_difference < self.score_difference_negative_threshold:\n",
    "                # normalized_reward = -1\n",
    "                normalized_reward = max(-10, min(10, score_difference))\n",
    "            else:\n",
    "                normalized_reward = 0\n",
    "        else:\n",
    "            normalized_reward = 20\n",
    "\n",
    "        return normalized_obs, normalized_reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_string='transferService'\n",
    "env= get_env(env_string='transferService')\n",
    "\n",
    "# # Load from disk\n",
    "data = np.load('obs_stats.npz')\n",
    "obs_min = data['min']\n",
    "obs_max = data['max']\n",
    "\n",
    "print(\"obs_min after loading\", obs_min)\n",
    "print(\"obs_max after loading\", obs_max)\n",
    "\n",
    "data = np.load('reward_stats.npz')\n",
    "reward_mean = data['mean']\n",
    "reward_std = data['std']\n",
    "\n",
    "print(\"reward_mean after loading\", reward_mean)\n",
    "print(\"reward_std after loading\", reward_std)\n",
    "\n",
    "# env = NormalizeObservationAndRewardWrapper(env, obs_mean, obs_std, reward_scale=1.0)\n",
    "env = NormalizeObservationAndRewardWrapper(env, obs_min, obs_max, reward_scale=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
