{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7675ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-mlw_s348 because the default path (/home/cc/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from utils import plot_learning_curve\n",
    "import sys\n",
    "from gym import spaces\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "import logging as log\n",
    "import time\n",
    "from pathlib import Path\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback,CheckpointCallback,CallbackList\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = str(Path(__file__).resolve().parent.parent)\n",
    "sys.path.append(parent_dir)\n",
    "from transferService import transferService\n",
    "from optimizer_gd import *\n",
    "from transferClass import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b19e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservationAndRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, obs_mean, obs_std, reward_scale=1.0):\n",
    "        super().__init__(env)\n",
    "        self.obs_mean = obs_mean\n",
    "        self.obs_std = obs_std\n",
    "        self.reward_scale = reward_scale\n",
    "        self.old_score=0\n",
    "        self.score_difference_positive_threadhold=2\n",
    "        self.score_difference_negative_threadhold=-2\n",
    "        self.observation_list=[]\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        # observation, reward, done, _, info = self.env.step(action)\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.observation_list.append(observation)\n",
    "        normalized_obs = self.normalize_observation(observation)\n",
    "        if reward != 1000000:\n",
    "        #   normalized_reward = reward * self.reward_scale\n",
    "            score_difference = reward - self.old_score\n",
    "            self.old_score = reward\n",
    "            if score_difference > self.score_difference_positive_threadhold:\n",
    "                normalized_reward = 1\n",
    "            elif score_difference < self.score_difference_negative_threadhold:\n",
    "                normalized_reward = -1\n",
    "            else:\n",
    "                normalized_reward = 0\n",
    "        else:\n",
    "            normalized_reward = 5\n",
    "        # return normalized_obs, normalized_reward, done, _ ,info\n",
    "        return normalized_obs, normalized_reward, done, info\n",
    "\n",
    "    def normalize_observation(self, observation):\n",
    "      # try:\n",
    "      #   return (observation - self.obs_mean) / self.obs_std\n",
    "      # except:\n",
    "      #   return np.zeros(40,)\n",
    "        EPSILON = 1e-10  # Small constant to prevent division by zero\n",
    "        normalized_observation = (observation - self.obs_mean) / (self.obs_std + EPSILON)\n",
    "        return normalized_observation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_string = 'transferService'\n",
    "    optimizer='ppo_sb3_10s_reward_by_difference'\n",
    "    for handler in log.root.handlers[:]:\n",
    "        log.root.removeHandler(handler)\n",
    "    log_FORMAT = '%(created)f -- %(levelname)s: %(message)s'\n",
    "    extraString=\"logFile\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = f\"./logFileDir/{optimizer}/\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    log_file = f\"logFileDir/{optimizer}/{optimizer}_{extraString}_{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
    "    log.basicConfig(\n",
    "        format=log_FORMAT,\n",
    "        datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "        level=log.INFO,\n",
    "        handlers=[\n",
    "            log.FileHandler(log_file),\n",
    "            # log.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    REMOTE_IP = \"129.114.109.46\"\n",
    "    REMOTE_PORT = \"80\"\n",
    "    INTERVAL = 1\n",
    "    INTERFACE = \"eno1\"\n",
    "    SERVER_IP = '127.0.0.1'\n",
    "    SERVER_PORT = 8080\n",
    "    transfer_service = transferService(REMOTE_IP, REMOTE_PORT, INTERVAL, INTERFACE, SERVER_IP, SERVER_PORT, optimizer, log)\n",
    "    env = transferClass(transfer_service,optimizer)\n",
    "\n",
    "    data = np.load('obs_stats.npz')\n",
    "    obs_mean = data['mean']\n",
    "    obs_std = data['std']\n",
    "\n",
    "    print(\"obs_mean after loading\", obs_mean)\n",
    "    print(\"obs_std after loading\", obs_std)\n",
    "\n",
    "    env = NormalizeObservationAndRewardWrapper(env, obs_mean, obs_std, reward_scale=1.0)\n",
    "\n",
    "    policy_kwargs = dict(activation_fn=th.nn.ReLU,net_arch=[{'pi': [128, 128], 'vf': [128, 128]}])\n",
    "    # model = PPO(\"MlpPolicy\", env=env, policy_kwargs=policy_kwargs, verbose=1,tensorboard_log=\"./ppo_tensorboard/\",ent_coef=0.01)\n",
    "\n",
    "\n",
    "    # model = PPO.load(\"./ppo_checkpoints/ppo_model_4000_steps.zip\")\n",
    "    model = PPO.load(\"./ppo_best_model/best_model.zip\")\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # action = env.action_space.sample()\n",
    "        print(\"action: \",action)\n",
    "        # obs, reward, done, info = env.step([action])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        print(\"obs: \", obs,\".... reward: \",reward)\n",
    "        # episode_reward += reward[0]\n",
    "        episode_reward += reward\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_num = 0\n",
    "    while run_num < 1:\n",
    "        main()\n",
    "        run_num += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
